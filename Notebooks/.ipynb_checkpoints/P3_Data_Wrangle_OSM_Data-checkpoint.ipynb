{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project 3: Data Wrangling Open Streetmaps Data\n",
    "##1. Problems encountered in the map\n",
    "####I decided to go for my hometown of Oslo, Norway, and naturally there was a bit of trouble with the non-English characters found there. I made two translation mappings, one for unicode and one for normal strings, for both small and capital æ,ø,å.\n",
    "\n",
    "### Non-English characters\n",
    "#### The non-English characters in the dataset have different encodings, both unicode and standard. This called for two different approaches to translating these to the standard English \n",
    "#### Other than this I didn't really find any particular problems, both post codes and house numbers seem to be within an acceptabel range (The post codes are all from the correct area).\n",
    "##2. Overview of the data\n",
    "\n",
    "###File sizes\n",
    "#### oslo_norway.osm .. 1,09 GB\n",
    "#### oslo_norwa.osm.json .. 3,014 KB\n",
    "\n",
    "###Some selected data\n",
    "#### The number of documents in the set is 14,754, 14,077 of which are nodes and the remaining 677 are ways.\n",
    "db.tranby.find().count()\n",
    "list(db.tranby.aggregate(nodes_and_ways))\n",
    "\n",
    "nodes_and_ways = [{\"\\$group\":{\"_id\":\"\\$type\",\"count\":{\"\\$sum\":1 \\}\\}\\},{\"$sort\":{\"count\":-1}}]\n",
    "\n",
    "#### There are 48 distinct contributors, the most active being 'vibrog', who's commited 5,310 (36%)of the 14,754 documents.\n",
    "len(db.tranby.distinct('created.user'))\n",
    "list(db.tranby.aggregate(top_user))\n",
    "\n",
    "top_user = [{\"\\$group\":{\"_id\":\"\\$created.user\",\"count\":{\"\\$sum\":1\\}\\}\\},{\"\\$sort\":{\"count\":-1}},{\"\\$limit\":1}]\n",
    "                           \n",
    "#### There are 92 distinct street addresses in the set.\n",
    "len(db.tranby.distinct('address.street'))\n",
    "\n",
    "##3. Other ideas about the dataset\n",
    "###Addresses per postal code\n",
    "####Though it would be interesting to see the number of addresses per postal code.\n",
    "####Results were as follows:\n",
    "####3409: 928\n",
    "####3408: 647\n",
    "####3420: 562\n",
    "####3406: 173\n",
    "####3403: 20\n",
    "#### using the aggregation\n",
    "addresses_per_postal_code = [{\"\\$match\":{\"address\":{\"\\$exists\": True\\}\\}\\},{\"\\$group\":{\"_id\":\"\\$address.postcode\",\"count\":{\"\\$sum\":1}}},{\"\\$sort\":{\"count\":-1}\\}\\]\n",
    "##4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "file_path = 'C:\\Users\\hakon.tromborg\\Data Analyst Nanodegree\\Data\\\\oslo_norway.osm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "#json->MongoDB can't handle the Norwegian letters, so I'm mapping them to their official [a-z] equivalents for both unicode\n",
    "#and normal strings\n",
    "letter_map = {u\"Å\":'Aa',\n",
    "              u\"Ø\":'Oe',\n",
    "              u\"Æ\":'Ae',\n",
    "              u\"å\":\"aa\",\n",
    "              u\"ø\":\"oe\",\n",
    "              u\"æ\":\"ae\"}\n",
    "unicode_map = {u\"\\xc5\":'Aa',\n",
    "              u\"\\xd8\":'Oe',\n",
    "              u\"\\xc6\":'Ae',\n",
    "              u\"\\xe5\":\"aa\",\n",
    "              u\"\\xf8\":\"oe\",\n",
    "              u\"\\xe6\":\"ae\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#From chapter 6\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    #data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                #data.append(el)\n",
    "                fo.write(json.dumps(el) + \"\\n\")\n",
    "                element.clear()\n",
    "        #fo.write(json.dumps(data) + \"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    node = {}\n",
    "    \n",
    "    node['created'] = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['type'] = element.tag\n",
    "        if element.tag == \"way\":\n",
    "            node['node_refs'] = []\n",
    "            for it in element.iter(\"nd\"):\n",
    "                node['node_refs'].append(it.attrib['ref'])\n",
    "\n",
    "        for a in element.attrib:\n",
    "            if a in CREATED:\n",
    "                node['created'][a] = element.attrib[a]\n",
    "            elif a == 'pos' or 'lat':\n",
    "                #Ensure 'pos' only shows up in documents with a latitude/longitude\n",
    "                if 'pos' not in node:\n",
    "                    node['pos'] = [\"0\",\"0\"]\n",
    "                if a == \"lat\":\n",
    "                    node['pos'][0]=(float(element.attrib[a]))\n",
    "                elif a == \"lon\":\n",
    "                    node['pos'][1]=(float(element.attrib[a]))\n",
    "            else:\n",
    "                node[a] = element.attrib[a]\n",
    "                \n",
    "\n",
    "        for it in element.iter(\"tag\"):\n",
    "            #Skip the iteration and drop the element if problemchars are discovered\n",
    "            e = problemchars.search(it.attrib['k'])\n",
    "            if e:\n",
    "                continue\n",
    "                \n",
    "            elif it.attrib['k'][:5] == \"addr:\":\n",
    "                if 'address' not in node:\n",
    "                    node['address'] = {}\n",
    "                c = it.attrib['k'].split(':')\n",
    "                \n",
    "                #If c is 2, there's exactly one : in the string, which is the format we're looking for\n",
    "                if len(c) == 2:\n",
    "                    translated_string = translate_string(it.attrib['v'])\n",
    "                    #Translating Norwegian letters proved more difficult than expected \n",
    "                    #because the set contains both unicode and normal strings\n",
    "                    node['address'][it.attrib['k'][5:]] = translated_string\n",
    "            else:\n",
    "                node[it.attrib['k']] = it.attrib['v']\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def translate_string(str):\n",
    "    if type(str) is unicode:\n",
    "        translated_string = translate_unicode(str)\n",
    "        return translated_string\n",
    "    else:\n",
    "        translated_string = translate_utf(str)\n",
    "        return translated_string\n",
    "    \n",
    "def translate_unicode(addr):\n",
    "    for l in unicode_map:\n",
    "        if l in list(addr):\n",
    "            #using if l in letter_map crashes with unicode characters because illegal decodings are attempted\n",
    "            addr = [c.replace(l, unicode_map[l]) for c in list(addr)]\n",
    "            addr = ''.join(addr)\n",
    "    return addr\n",
    "def translate_utf(addr):\n",
    "    for l in letter_map:\n",
    "        if l in addr:\n",
    "            addr = addr.replace(l,letter_map[l])\n",
    "    return addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file():\n",
    "    data = process_map(file_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "process_file()\n",
    "print time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_data(data, db):\n",
    "    db.tranby.insert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:\n",
      "14754\n",
      "Number of nodes and ways:\n",
      "[{u'_id': u'node', u'count': 14077}, {u'_id': u'way', u'count': 677}]\n",
      "Number of unique users:\n",
      "48\n",
      "Top contributing user:\n",
      "[{u'count': 5310, u'_id': u'vibrog'}]\n",
      "Distinct street addresses in set:\n",
      "92\n",
      "Number of addresses per postal code: \n",
      "[{u'count': 928, u'_id': u'3409'}, {u'count': 647, u'_id': u'3408'}, {u'count': 562, u'_id': u'3420'}, {u'count': 173, u'_id': u'3406'}, {u'count': 20, u'_id': u'3403'}]\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.tranby\n",
    "#Clear the database so it doesn't add multiple records\n",
    "db.tranby.remove()\n",
    "\n",
    "with open(file_path + '.json') as f:\n",
    "        data = json.loads(f.read())\n",
    "        insert_data(data, db)\n",
    "nodes_and_ways = [{\"$group\":{\"_id\":\"$type\",\n",
    "                      \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}}]\n",
    "\n",
    "top_user = [{\"$group\":{\"_id\":\"$created.user\",\n",
    "                            \"count\":{\"$sum\":1}}},\n",
    "                           {\"$sort\":{\"count\":-1}},\n",
    "                           {\"$limit\":1}]\n",
    "addresses_per_postal_code = [{\"$match\":{\"address\":{\"$exists\": True}}},\n",
    "                            {\"$group\":{\"_id\":\"$address.postcode\",\n",
    "                            \"count\":{\"$sum\":1}}},\n",
    "                           {\"$sort\":{\"count\":-1}}]\n",
    "\n",
    "print \"Number of records:\"\n",
    "print db.tranby.find().count()\n",
    "\n",
    "print \"Number of nodes and ways:\"\n",
    "pprint.pprint(list(db.tranby.aggregate(nodes_and_ways)))\n",
    "\n",
    "print \"Number of unique users:\"\n",
    "print len(db.tranby.distinct('created.user'))\n",
    "\n",
    "print \"Top contributing user:\"\n",
    "print list(db.tranby.aggregate(top_user))\n",
    "\n",
    "print \"Distinct street addresses in set:\"\n",
    "print len(db.tranby.distinct('address.street'))\n",
    "\n",
    "print \"Number of addresses per postal code: \"\n",
    "print list(db.tranby.aggregate(addresses_per_postal_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 0 <= num <= 0, not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-291-10e1b18c118c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#print d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\hakon.tromborg\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc\u001b[0m in \u001b[0;36mhist_frame\u001b[1;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, **kwds)\u001b[0m\n\u001b[0;32m   2764\u001b[0m     fig, axes = _subplots(naxes=naxes, ax=ax, squeeze=False,\n\u001b[0;32m   2765\u001b[0m                           \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msharex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msharey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2766\u001b[1;33m                           layout=layout)\n\u001b[0m\u001b[0;32m   2767\u001b[0m     \u001b[0m_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hakon.tromborg\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\tools\\plotting.pyc\u001b[0m in \u001b[0;36m_subplots\u001b[1;34m(naxes, sharex, sharey, squeeze, subplot_kw, ax, layout, layout_type, **fig_kw)\u001b[0m\n\u001b[0;32m   3244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3245\u001b[0m     \u001b[1;31m# Create first subplot separately, so we can share it if requested\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3246\u001b[1;33m     \u001b[0max0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hakon.tromborg\\AppData\\Roaming\\Python\\Python27\\site-packages\\matplotlib\\figure.pyc\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\hakon.tromborg\\AppData\\Roaming\\Python\\Python27\\site-packages\\matplotlib\\axes\\_subplots.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m                     raise ValueError(\n\u001b[0;32m     63\u001b[0m                         \"num must be 0 <= num <= {maxn}, not {num}\".format(\n\u001b[1;32m---> 64\u001b[1;33m                             maxn=rows*cols, num=num))\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                     warnings.warn(\"The use of 0 (which ends up being the \"\n",
      "\u001b[1;31mValueError\u001b[0m: num must be 0 <= num <= 0, not 1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "times = db.tranby.distinct('created.timestamp')\n",
    "for x,t in enumerate(times):\n",
    "    times[x] = times[x][:10]\n",
    "#print sorted(times)\n",
    "d = pd.DataFrame(sorted(times))\n",
    "#print d\n",
    "d.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
